{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, pipeline, set_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformer.wte.weight torch.Size([50257, 768])\n",
      "transformer.wpe.weight torch.Size([1024, 768])\n",
      "transformer.h.0.ln_1.weight torch.Size([768])\n",
      "transformer.h.0.ln_1.bias torch.Size([768])\n",
      "transformer.h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.0.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.0.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.0.ln_2.weight torch.Size([768])\n",
      "transformer.h.0.ln_2.bias torch.Size([768])\n",
      "transformer.h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.0.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_1.weight torch.Size([768])\n",
      "transformer.h.1.ln_1.bias torch.Size([768])\n",
      "transformer.h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.1.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.1.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.1.ln_2.weight torch.Size([768])\n",
      "transformer.h.1.ln_2.bias torch.Size([768])\n",
      "transformer.h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.1.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_1.weight torch.Size([768])\n",
      "transformer.h.2.ln_1.bias torch.Size([768])\n",
      "transformer.h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.2.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.2.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.2.ln_2.weight torch.Size([768])\n",
      "transformer.h.2.ln_2.bias torch.Size([768])\n",
      "transformer.h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.2.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_1.weight torch.Size([768])\n",
      "transformer.h.3.ln_1.bias torch.Size([768])\n",
      "transformer.h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.3.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.3.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.3.ln_2.weight torch.Size([768])\n",
      "transformer.h.3.ln_2.bias torch.Size([768])\n",
      "transformer.h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.3.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_1.weight torch.Size([768])\n",
      "transformer.h.4.ln_1.bias torch.Size([768])\n",
      "transformer.h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.4.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.4.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.4.ln_2.weight torch.Size([768])\n",
      "transformer.h.4.ln_2.bias torch.Size([768])\n",
      "transformer.h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.4.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_1.weight torch.Size([768])\n",
      "transformer.h.5.ln_1.bias torch.Size([768])\n",
      "transformer.h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.5.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.5.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.5.ln_2.weight torch.Size([768])\n",
      "transformer.h.5.ln_2.bias torch.Size([768])\n",
      "transformer.h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.5.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_1.weight torch.Size([768])\n",
      "transformer.h.6.ln_1.bias torch.Size([768])\n",
      "transformer.h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.6.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.6.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.6.ln_2.weight torch.Size([768])\n",
      "transformer.h.6.ln_2.bias torch.Size([768])\n",
      "transformer.h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.6.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_1.weight torch.Size([768])\n",
      "transformer.h.7.ln_1.bias torch.Size([768])\n",
      "transformer.h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.7.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.7.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.7.ln_2.weight torch.Size([768])\n",
      "transformer.h.7.ln_2.bias torch.Size([768])\n",
      "transformer.h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.7.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_1.weight torch.Size([768])\n",
      "transformer.h.8.ln_1.bias torch.Size([768])\n",
      "transformer.h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.8.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.8.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.8.ln_2.weight torch.Size([768])\n",
      "transformer.h.8.ln_2.bias torch.Size([768])\n",
      "transformer.h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.8.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_1.weight torch.Size([768])\n",
      "transformer.h.9.ln_1.bias torch.Size([768])\n",
      "transformer.h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.9.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.9.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.9.ln_2.weight torch.Size([768])\n",
      "transformer.h.9.ln_2.bias torch.Size([768])\n",
      "transformer.h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.9.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_1.weight torch.Size([768])\n",
      "transformer.h.10.ln_1.bias torch.Size([768])\n",
      "transformer.h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.10.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.10.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.10.ln_2.weight torch.Size([768])\n",
      "transformer.h.10.ln_2.bias torch.Size([768])\n",
      "transformer.h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.10.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_1.weight torch.Size([768])\n",
      "transformer.h.11.ln_1.bias torch.Size([768])\n",
      "transformer.h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "transformer.h.11.attn.c_attn.bias torch.Size([2304])\n",
      "transformer.h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "transformer.h.11.attn.c_proj.bias torch.Size([768])\n",
      "transformer.h.11.ln_2.weight torch.Size([768])\n",
      "transformer.h.11.ln_2.bias torch.Size([768])\n",
      "transformer.h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "transformer.h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "transformer.h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "transformer.h.11.mlp.c_proj.bias torch.Size([768])\n",
      "transformer.ln_f.weight torch.Size([768])\n",
      "transformer.ln_f.bias torch.Size([768])\n",
      "lm_head.weight torch.Size([50257, 768])\n"
     ]
    }
   ],
   "source": [
    "model_hf = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "sd_hf = model_hf.state_dict()\n",
    "\n",
    "for k, v in sd_hf.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Hello, I\\'m a language model, so you can\\'t define something in any other language. Let me introduce another topic:\\n\\nThe name \"'},\n",
       " {'generated_text': \"Hello, I'm a language model, you know.\\n\\nThat's right… I have a lot of friends who don't know what I do\"},\n",
       " {'generated_text': \"Hello, I'm a language model, not a formal one. I'm more interested in languages than formal models and I'm going to use the formal\"},\n",
       " {'generated_text': 'Hello, I\\'m a language model, but there are lots of people out there who think what I did was wrong.\\n\\n\"I do believe'},\n",
       " {'generated_text': \"Hello, I'm a language model, and I can understand my own language. But in order for me to be an artist, I have to look\"}]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# hf transformer example\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "set_seed(42)\n",
    "generator(\"Hello, I'm a language model,\", max_length=30, num_return_sequences=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0: '!'\n",
      "Token 1: '\"'\n",
      "Token 2: '#'\n",
      "Token 3: '$'\n",
      "Token 4: '%'\n",
      "Token 5: '&'\n",
      "Token 6: \"'\"\n",
      "Token 7: '('\n",
      "Token 8: ')'\n",
      "Token 9: '*'\n",
      "Token 10: '+'\n",
      "Token 11: ','\n",
      "Token 12: '-'\n",
      "Token 13: '.'\n",
      "Token 14: '/'\n",
      "Token 15: '0'\n",
      "Token 16: '1'\n",
      "Token 17: '2'\n",
      "Token 18: '3'\n",
      "Token 19: '4'\n",
      "Token 20: '5'\n",
      "Token 21: '6'\n",
      "Token 22: '7'\n",
      "Token 23: '8'\n",
      "Token 24: '9'\n",
      "Token 25: ':'\n",
      "Token 26: ';'\n",
      "Token 27: '<'\n",
      "Token 28: '='\n",
      "Token 29: '>'\n",
      "Token 30: '?'\n",
      "Token 31: '@'\n",
      "Token 32: 'A'\n",
      "Token 33: 'B'\n",
      "Token 34: 'C'\n",
      "Token 35: 'D'\n",
      "Token 36: 'E'\n",
      "Token 37: 'F'\n",
      "Token 38: 'G'\n",
      "Token 39: 'H'\n",
      "Token 40: 'I'\n",
      "Token 41: 'J'\n",
      "Token 42: 'K'\n",
      "Token 43: 'L'\n",
      "Token 44: 'M'\n",
      "Token 45: 'N'\n",
      "Token 46: 'O'\n",
      "Token 47: 'P'\n",
      "Token 48: 'Q'\n",
      "Token 49: 'R'\n",
      "Token 50: 'S'\n",
      "Token 51: 'T'\n",
      "Token 52: 'U'\n",
      "Token 53: 'V'\n",
      "Token 54: 'W'\n",
      "Token 55: 'X'\n",
      "Token 56: 'Y'\n",
      "Token 57: 'Z'\n",
      "Token 58: '['\n",
      "Token 59: '\\\\'\n",
      "Token 60: ']'\n",
      "Token 61: '^'\n",
      "Token 62: '_'\n",
      "Token 63: '`'\n",
      "Token 64: 'a'\n",
      "Token 65: 'b'\n",
      "Token 66: 'c'\n",
      "Token 67: 'd'\n",
      "Token 68: 'e'\n",
      "Token 69: 'f'\n",
      "Token 70: 'g'\n",
      "Token 71: 'h'\n",
      "Token 72: 'i'\n",
      "Token 73: 'j'\n",
      "Token 74: 'k'\n",
      "Token 75: 'l'\n",
      "Token 76: 'm'\n",
      "Token 77: 'n'\n",
      "Token 78: 'o'\n",
      "Token 79: 'p'\n",
      "Token 80: 'q'\n",
      "Token 81: 'r'\n",
      "Token 82: 's'\n",
      "Token 83: 't'\n",
      "Token 84: 'u'\n",
      "Token 85: 'v'\n",
      "Token 86: 'w'\n",
      "Token 87: 'x'\n",
      "Token 88: 'y'\n",
      "Token 89: 'z'\n",
      "Token 90: '{'\n",
      "Token 91: '|'\n",
      "Token 92: '}'\n",
      "Token 93: '~'\n",
      "Token 94: '�'\n",
      "Token 95: '�'\n",
      "Token 96: '�'\n",
      "Token 97: '�'\n",
      "Token 98: '�'\n",
      "Token 99: '�'\n",
      "Token 100: '�'\n",
      "Token 101: '�'\n",
      "Token 102: '�'\n",
      "Token 103: '�'\n",
      "Token 104: '�'\n",
      "Token 105: '�'\n",
      "Token 106: '�'\n",
      "Token 107: '�'\n",
      "Token 108: '�'\n",
      "Token 109: '�'\n",
      "Token 110: '�'\n",
      "Token 111: '�'\n",
      "Token 112: '�'\n",
      "Token 113: '�'\n",
      "Token 114: '�'\n",
      "Token 115: '�'\n",
      "Token 116: '�'\n",
      "Token 117: '�'\n",
      "Token 118: '�'\n",
      "Token 119: '�'\n",
      "Token 120: '�'\n",
      "Token 121: '�'\n",
      "Token 122: '�'\n",
      "Token 123: '�'\n",
      "Token 124: '�'\n",
      "Token 125: '�'\n",
      "Token 126: '�'\n",
      "Token 127: '�'\n",
      "Token 128: '�'\n",
      "Token 129: '�'\n",
      "Token 130: '�'\n",
      "Token 131: '�'\n",
      "Token 132: '�'\n",
      "Token 133: '�'\n",
      "Token 134: '�'\n",
      "Token 135: '�'\n",
      "Token 136: '�'\n",
      "Token 137: '�'\n",
      "Token 138: '�'\n",
      "Token 139: '�'\n",
      "Token 140: '�'\n",
      "Token 141: '�'\n",
      "Token 142: '�'\n",
      "Token 143: '�'\n",
      "Token 144: '�'\n",
      "Token 145: '�'\n",
      "Token 146: '�'\n",
      "Token 147: '�'\n",
      "Token 148: '�'\n",
      "Token 149: '�'\n",
      "Token 150: '�'\n",
      "Token 151: '�'\n",
      "Token 152: '�'\n",
      "Token 153: '�'\n",
      "Token 154: '�'\n",
      "Token 155: '�'\n",
      "Token 156: '�'\n",
      "Token 157: '�'\n",
      "Token 158: '�'\n",
      "Token 159: '�'\n",
      "Token 160: '�'\n",
      "Token 161: '�'\n",
      "Token 162: '�'\n",
      "Token 163: '�'\n",
      "Token 164: '�'\n",
      "Token 165: '�'\n",
      "Token 166: '�'\n",
      "Token 167: '�'\n",
      "Token 168: '�'\n",
      "Token 169: '�'\n",
      "Token 170: '�'\n",
      "Token 171: '�'\n",
      "Token 172: '�'\n",
      "Token 173: '�'\n",
      "Token 174: '�'\n",
      "Token 175: '�'\n",
      "Token 176: '�'\n",
      "Token 177: '�'\n",
      "Token 178: '�'\n",
      "Token 179: '�'\n",
      "Token 180: '�'\n",
      "Token 181: '�'\n",
      "Token 182: '�'\n",
      "Token 183: '�'\n",
      "Token 184: '�'\n",
      "Token 185: '�'\n",
      "Token 186: '�'\n",
      "Token 187: '�'\n",
      "Token 188: '\\x00'\n",
      "Token 189: '\\x01'\n",
      "Token 190: '\\x02'\n",
      "Token 191: '\\x03'\n",
      "Token 192: '\\x04'\n",
      "Token 193: '\\x05'\n",
      "Token 194: '\\x06'\n",
      "Token 195: '\\x07'\n",
      "Token 196: '\\x08'\n",
      "Token 197: '\\t'\n",
      "Token 198: '\\n'\n",
      "Token 199: '\\x0b'\n",
      "Token 200: '\\x0c'\n",
      "Token 201: '\\r'\n",
      "Token 202: '\\x0e'\n",
      "Token 203: '\\x0f'\n",
      "Token 204: '\\x10'\n",
      "Token 205: '\\x11'\n",
      "Token 206: '\\x12'\n",
      "Token 207: '\\x13'\n",
      "Token 208: '\\x14'\n",
      "Token 209: '\\x15'\n",
      "Token 210: '\\x16'\n",
      "Token 211: '\\x17'\n",
      "Token 212: '\\x18'\n",
      "Token 213: '\\x19'\n",
      "Token 214: '\\x1a'\n",
      "Token 215: '\\x1b'\n",
      "Token 216: '\\x1c'\n",
      "Token 217: '\\x1d'\n",
      "Token 218: '\\x1e'\n",
      "Token 219: '\\x1f'\n",
      "Token 220: ' '\n",
      "Token 221: '\\x7f'\n",
      "Token 222: '�'\n",
      "Token 223: '�'\n",
      "Token 224: '�'\n",
      "Token 225: '�'\n",
      "Token 226: '�'\n",
      "Token 227: '�'\n",
      "Token 228: '�'\n",
      "Token 229: '�'\n",
      "Token 230: '�'\n",
      "Token 231: '�'\n",
      "Token 232: '�'\n",
      "Token 233: '�'\n",
      "Token 234: '�'\n",
      "Token 235: '�'\n",
      "Token 236: '�'\n",
      "Token 237: '�'\n",
      "Token 238: '�'\n",
      "Token 239: '�'\n",
      "Token 240: '�'\n",
      "Token 241: '�'\n",
      "Token 242: '�'\n",
      "Token 243: '�'\n",
      "Token 244: '�'\n",
      "Token 245: '�'\n",
      "Token 246: '�'\n",
      "Token 247: '�'\n",
      "Token 248: '�'\n",
      "Token 249: '�'\n",
      "Token 250: '�'\n",
      "Token 251: '�'\n",
      "Token 252: '�'\n",
      "Token 253: '�'\n",
      "Token 254: '�'\n",
      "Token 255: '�'\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for i in range(256):\n",
    "    print(f\"Token {i}: {repr(enc.decode([i]))}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
